---
layout: base
---

<h3><a name="about" class="anchor" href="#about"><span class="glyphicon glyphicon-link"></span></a> About</h3>
<p>Zephyr was the genesis of porting a proprietary, real time, single node ETL process to Storm. Through work done on the previous system, the generic steps in a common ETL
    process was clearly defined;
<ol>
    <li>Initial Data enters system</li>
    <li>Pre-process</li>
    <li>Parse</li>
    <li>Normalize, Validate, and Canonicalize</li>
    <li>Enrich</li>
    <li>Output</li>
</ol>
With the exception of output, all of these steps are BigData platform agnostic.</p>

<h3><a name="who" class="anchor" href="#who"><span class="glyphicon glyphicon-link"></span></a> Who can use zephyr?</h3>
<p>You essentially need to be a developer to use Zephyr, because it requires knowledge of ETL, Java, MapReduce, and distributed systems.</p>

<h3><a name="why" class="anchor" href="#why"><span class="glyphicon glyphicon-link"></span></a> I am a developer! Why should I use it?</h3>
<p>Have you ever had a data set that required tuning before it could be operated on? Zephyr is a tool that distributes your data and transforms it into a format of your choosing.  The zephyr-core and
    zephyr-mapreduce projects do the heavy lifting for you.  You only need to write the schema.  We already have a few parsers written that might work for you already.  If our parsers and pre-processors
    don't work for you, then you can always write your own.  We have interfaces that you can extend to create your own parsers, normalizers, pre-processors, enrichers, and validators.</p>

<h3><a name="design-goals" class="anchor" href="#design-goals"><span class="glyphicon glyphicon-link"></span></a> Design Goals</h3>
<p>Zephyr was conceived as a way to formalize these steps in a Java API for use in any of the most prominent (or promising) BigData processing platforms. Steps 2-5 in the above steps could be
    created irrespective of the platform they would run in for a given feed, with steps 1 and 6 being determined for the chosen processing platform. <b>Above all else, the vast majority of work done to your
        data
        should be dealt with through Zephyr's Core library (or your extensions to that).</b> Further, by following this pattern, it is overwhelmingly likely that steps 1 and 6 should be solved for you
    right out of the box by using the Zephyr MapReduce or Zephyr Storm (or Spark Streaming) implementations. </p>
<h5>Spring configuration</h5>
<p>Spring is used for the configuration of a Zephyr Schema - the normalization, validation, and canonicalization phase. It's also used as the final glue for a Zephyr MapReduce job - it pulls together steps 1
    through 6 listed above
    and makes your job easily configurable at runtime - provided no new parsers, enrichers, or outputters are required. However, even if these are required, you need only build your own jar and drop it into the
    Zephyr lib directory and
    refer to the appropriate classes in the Spring configuration file. To see more, go to the <a href="/zephyr/Configuration/">Zephyr Configuration</a> page for more information.</p>

<h3><a name="status" class="anchor" href="#status"><span class="glyphicon glyphicon-link"></span></a> Status</h3>
<p>After Zephyr's core library was completed, focus shifted from a rigid, hardcoded Zephyr topology to a flexible, configurable MapReduce job instead.
    By chaining the Zephyr ETL process in a map task, we're able to achieve the same process as we did with Storm. Business needs shifted, and primary development
    of Zephyr focused on MapReduce only, with the Storm branch falling by the wayside.</p>
<p>While real time development did not continue at the same pace as batch, the core library continues to support either - or even a standalone process.</p>

<h3><a name="contact" class="anchor" href="#contact"><span class="glyphicon glyphicon-link"></span></a> Contact</h3>
<p>Please feel free to contact <a href="mailto:dwayne.pryce@gmail.com">Dwayne Pryce</a> with any questions you might have about Zephyr.</p>