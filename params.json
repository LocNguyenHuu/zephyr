{"name":"Zephyr","tagline":"Zephyr is a big data, platform agnostic ETL API, with Hadoop MapReduce, Storm, and other big data bindings.","body":"Zephyr - ETL at Scale\r\n========================\r\n\r\nCurrent Status\r\n-----------------------\r\nInitial Release 0.1.0!\r\n - Storm and Spark Streaming are going to go in their own branches and not be included in this release - checkout branch storm-dev and (eventually) spark-streaming-dev for a peek at these!\r\n\r\nWhy?\r\n------------------------\r\nETL - Extract, Transform, and Load - has been around for decades.  The concept of taking data in one form, \r\nextracting the important bits, transforming them into data you actually want to use, and loading it into some\r\ndestination is hardly virgin ground.  However, for the last few years, over innumerable projects, I've seen\r\nso many one-off, rigid implementations of ETL - often laden with bugs, poor performance, and rarely scaleable.\r\n\r\nThe problem is a pure engineering problem.  What is the most efficient way to get data from source, to destination,\r\nwhen we're dealing with gigabytes or terabytes of data?  Further, what if our pipeline changes - from batch, to \r\nstreaming -- or streaming, to batch?\r\n\r\nRather than watch everyone recreate the wheel in a very specific context, and then recreate it again when \r\nbusiness needs or platforms change - I wrote Zephyr, an API, an abstraction, for ETL at BigData scale.\r\n\r\nBig Data Implementations\r\n------------------------\r\nCurrently, Zephyr supports the following implementations (as in, your code, when written to the Zephyr API, \r\nwill not need to be recompiled to work with):\r\n* <a href=\"http://hadoop.apache.org\">Hadoop MapReduce</a> (<a href=\"http://www.cloudera.com\">CDH 4.3.0</a>)\r\n* <a href=\"http://storm-project.net\">Storm</a>\r\n\r\nWe also fully plan to support <a href=\"http://spark-project.org\">Spark Streaming</a> - and much of our \r\nwork for Storm (in  terms of initially loading the data into the platform, and writing the data to our \r\ndestination) will help us implement this in a fairly short period of time.\r\n\r\nGeneral Flow\r\n-------------------------\r\nBy and large, each of our targeted scaleable implementations has its own way (external to Zephyr) for best\r\ngetting data into their system.  Whether it is reading from some sort of InputFormat in MapReduce, or a\r\nspecific queue implementation (Kafka,JMS) -- or even just a socket connection.  These are all considered to be\r\noutside the scope of Zephyr.  Far be it from us to tell you how your data gets to your BigData processing\r\nimplementation.\r\n\r\nHowever, once it is available to MapReduce/Storm - Zephyr takes over.  It provides a series of contracts to \r\nbe fulfilled by your implementations.  In order, they are:\r\n* Preprocessing (optional)\r\n* Parsing\r\n* Schema Mapping (including naming, validating, and normalizing)\r\n* Enrichment (0..N, in order, no control flow)\r\n\r\nYou may have noticed there is no output phase; that, like our input phase, is up to you.  For our MapReduce\r\nimplementation, we generally allow our Mapper to write the data out to HDFS (with no Reduce phase), and we\r\nare done.  Storm or Spark Streaming, on the other hand, would have their own ways of persisting your \r\nextracted and transformed data (such as an HBase writer).\r\n\r\nMost of our implementations provide helper methods for these Input and Output phases.  The important piece of\r\nZephyr - Zephyr-Core - is really the most vital part of the process.  We anticipate that, for some, the\r\nMapReduce implementation may fall short; whether it's via assumption of data coming in as either Text or\r\nBytesWritable, or that we want to write it to HDFS - and Zephyr has been written such that extension of\r\nour classes - or superceding them entirely (and using only Zephyr-Core) is both expected and embraced.\r\n\r\nOur goal for Zephyr and each of the BigData implementations is that it will accomodate 90% of ETL needs - \r\nor at least for structured data (free text or imagery/video ingestion are an entirely different beast - though \r\neither might still be well-served by utilizing Zephyr).\r\n\r\nUsing Zephyr in your Ingestion Product\r\n--------------------------\r\nRequirements:\r\n* JDK 6+ (you can use Java, Scala, Groovy, Clojure)\r\n* Gradle 1.5+\r\n\r\nZephyr will be built and published (as a jar) for our first release; at that point, you will merely need \r\nto include the gav org.zephyr:zephyr-<bigdata-implementation>:<version> - and that will include zephyr-core.\r\n\r\nYou may also want to include from contrib - helper classes such as a Secure Event (for real-time analytics with\r\nvisibility concerns), or a (extremely) simplistic HBase Outputter, will also be available for inclusion.\r\n\r\nMore importantly will be our Sample project, which will exist as source code and structure - this will offer \r\nyou a great example for building and running a Zephyr ETL job.  We invite you to download this project \r\n<here>.  We utilize gradle for our build and distribution creating scripts, and have included some run files.\r\n\r\nYou are welcome to extend or enhance these - or supercede them entirely.  If you feel that your solution\r\nis superior to the one bundled - please fork and submit a pull request!\r\n\r\nOther Resources\r\n--------------------------\r\n- <a href=\"https://github.com/Sotera/zephyr-sample-project\">Zephyr Sample Project(s)</a> - A sample project structure that shows off how you might use Zephyr\r\n- <a href=\"https://github.com/Sotera/zephyr-contrib\">Zephyr Contrib</a> - for projects that don't really belong in Zephyr, but can be used with Zephyr to add other capabilities (like Accumulo visibility controls)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}